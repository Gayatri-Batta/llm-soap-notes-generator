{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ef4f700a16448aea0bfd779070272c8",
            "60526cb2ce874e18b0317cfef69a2a35",
            "f417fa49b33544b8a3e1c85fca6f00fe",
            "31a7e41f2ca14049b6a782066372492b",
            "705544c2a786439b84b3ca01bb342105",
            "9be51213081f42fa9c2539d772eeb40f",
            "04ea4948e3e648729bd194aa63ebe9e1",
            "ae77e20ea7414f928d49b3484fb696f5",
            "eaf0d01ec4f2426fa7e0eb9f07bd3492",
            "39172cdda2ac44f8a78cb46cc3bd6c5e",
            "7441772a308247f59b5452541ae53fc2"
          ]
        },
        "id": "Fnol0gnsoxDd",
        "outputId": "1d001c0f-621b-4131-9547-804424e2d25a"
      },
      "outputs": [],
      "source": [
        "# # Transcript to SOAP Note Generation using m42-health/Llama3-Med42-8B (8-bit Transformers)\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"m42-health/Llama3-Med42-8B\"\n",
        "\n",
        "# Global variables for model and tokenizer\n",
        "LLM_PIPELINE = None\n",
        "TOKENIZER = None\n",
        "MODEL_LOADED_SUCCESSFULLY = False\n",
        "\n",
        "print(f\"Attempting to load model: {MODEL_NAME_OR_PATH}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    current_device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
        "    print(f\"Device name: {current_device_name}\")\n",
        "\n",
        "    # Check bfloat16 support for compute_dtype (optional but good for performance on compatible GPUs)\n",
        "    if \"cuda\" in str(torch.rand(1, device=\"cuda\").dtype): # Check if CUDA is working at all\n",
        "        try:\n",
        "            _ = torch.rand(1, dtype=torch.bfloat16, device=\"cuda\") * torch.rand(1, dtype=torch.bfloat16, device=\"cuda\")\n",
        "            BF16_SUPPORTED = True\n",
        "            print(f\"Device {current_device_name} appears to support bfloat16.\")\n",
        "        except RuntimeError:\n",
        "            BF16_SUPPORTED = False\n",
        "            print(f\"Device {current_device_name} does NOT appear to support bfloat16. Will use float16 for torch_dtype if CUDA available.\")\n",
        "    else:\n",
        "        BF16_SUPPORTED = False\n",
        "else:\n",
        "    BF16_SUPPORTED = False\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        print(\"Set tokenizer.pad_token_id to tokenizer.eos_token_id\")\n",
        "\n",
        "    # Determine torch_dtype based on availability\n",
        "    if torch.cuda.is_available():\n",
        "        model_dtype = torch.bfloat16 if BF16_SUPPORTED else torch.float16\n",
        "    else:\n",
        "        model_dtype = torch.float32 # Default for CPU\n",
        "    print(f\"Using torch_dtype: {model_dtype}\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME_OR_PATH,\n",
        "        load_in_8bit=True,      # Enable 8-bit quantization [3]\n",
        "        device_map=\"auto\",      # Automatically distribute model layers\n",
        "        torch_dtype=model_dtype # For non-quantized layers and computations\n",
        "    )\n",
        "    print(\"Model loaded successfully with 8-bit quantization.\")\n",
        "    print(f\"Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "    # Create the text-generation pipeline\n",
        "    llm_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    LLM_PIPELINE = llm_pipeline\n",
        "    TOKENIZER = tokenizer\n",
        "    MODEL_LOADED_SUCCESSFULLY = True\n",
        "    print(\"Text-generation pipeline created successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or creating pipeline: {e}\")\n",
        "    print(\"Please ensure 'bitsandbytes' and 'accelerate' are installed and compatible.\")\n",
        "    print(\"A GPU is highly recommended. Check CUDA setup and GPU memory.\")\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"CUDA out of memory. This 8B model, even 8-bit quantized, requires significant VRAM.\")\n",
        "    elif \"load_in_8bit\" in str(e):\n",
        "        print(\"Error related to 8-bit loading. Double-check library versions and GPU compatibility.\")\n",
        "\n",
        "\n",
        "\n",
        "TRANSCRIPTS_INPUT_DIR = \"./transcripts_data\"\n",
        "SOAP_OUTPUT_DIR = \"./soap_notes_output\"\n",
        "\n",
        "if not os.path.exists(SOAP_OUTPUT_DIR):\n",
        "    os.makedirs(SOAP_OUTPUT_DIR)\n",
        "    print(f\"Created directory: {SOAP_OUTPUT_DIR}\")\n",
        "\n",
        "PROCEED_WITH_PROCESSING = True\n",
        "if not os.path.exists(TRANSCRIPTS_INPUT_DIR):\n",
        "    print(f\"ERROR: Transcripts input directory '{TRANSCRIPTS_INPUT_DIR}' not found.\")\n",
        "    PROCEED_WITH_PROCESSING = False\n",
        "elif not os.listdir(TRANSCRIPTS_INPUT_DIR):\n",
        "    print(f\"WARNING: Transcripts input directory '{TRANSCRIPTS_INPUT_DIR}' is empty.\")\n",
        "    PROCEED_WITH_PROCESSING = False\n",
        "else:\n",
        "    print(f\"Transcripts will be read from: {TRANSCRIPTS_INPUT_DIR}\")\n",
        "\n",
        "TRANSCRIPT_FILENAMES = [\n",
        "    \"encounter_1.txt\", \"encounter_2.txt\", \"encounter_3.txt\", \"encounter_4.txt\", \"encounter_5.txt\", \"encounter_6.txt\", \"encounter_7.txt\", \"encounter_8.txt\", \"encounter_9.txt\", \"encounter_10.txt\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def load_transcript(file_path: str) -> str:\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Transcript file not found at {file_path}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def preprocess_transcript(transcript_text: str) -> str:\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', transcript_text).strip()\n",
        "    return re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "\n",
        "\n",
        "\n",
        "def create_llama3_soap_prompt(tokenizer, transcript_text: str, patient_name: str = \"Not specified in transcript\", patient_dob: str = \"Not specified in transcript\") -> str:\n",
        "    date_of_service = datetime.now().strftime(\"%Y-%m-%d\") # Standardized format\n",
        "\n",
        "    system_prompt_content = (\n",
        "        \"You are a helpful, respectful and honest medical assistant AI. You are a second version of Med42 developed by the AI team at M42, UAE. \"\n",
        "        \"Always answer as helpfully as possible, while being safe. \"\n",
        "        \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
        "        \"Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
        "        \"If you don’t know the answer to a question, please don’t share false information. \"\n",
        "        \"Your primary task is to generate a comprehensive and detailed clinical SOAP note based on the provided medical encounter transcript. \"\n",
        "        \"The transcript may have irregular formatting or missing speaker labels (e.g., 'Doctor:', 'Patient:'); you must infer roles and extract information from the dialogue flow. \"\n",
        "        \"Focus on accuracy and strict adherence to the detailed SOAP structure outlined below. Use only information explicitly present or directly and confidently inferable from the transcript. \"\n",
        "        \"Structure the output exactly as specified in the 'SOAP Note Format to Follow', including all headers and sub-headers.\"\n",
        "    )\n",
        "\n",
        "    user_instructions_content = f\"\"\"\n",
        "        **Medical Encounter Details (Extract from transcript if available, otherwise state \"Not mentioned in transcript\"):**\n",
        "        Client Full Name: {patient_name}\n",
        "        Client Date of Birth: {patient_dob}\n",
        "        Date of Service: {date_of_service}\n",
        "        Exact start time and end time: [Fill or state \"Not mentioned in transcript\"]\n",
        "        Session Location: [Fill or state \"Not mentioned in transcript\"]\n",
        "        Diagnosis (Primary, from assessment or clearly stated): [Fill or state \"To be determined by clinician\" or use assessment]\n",
        "\n",
        "        **Transcript to Process:**\n",
        "        \\\"\\\"\\\"\n",
        "        {transcript_text}\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        **Instructions for SOAP Note Generation:**\n",
        "        Based **only** on the information present in the transcript provided above, generate a detailed SOAP note.\n",
        "        If information for a specific field or sub-section is not present in the transcript, explicitly state \"Not mentioned in transcript\" or \"N/A\". Do not invent information.\n",
        "        Follow the structure and level of detail exemplified by high-quality medical SOAP notes.\n",
        "\n",
        "        **SOAP Note Format to Follow:**\n",
        "\n",
        "        **S (Subjective):**\n",
        "            - Chief Complaint (CC): (Patient's main reason for visit, ideally in their own words. E.g., \"Experiencing increased anxiety symptoms.\")\n",
        "            - History of Present Illness (HPI): (Detailed chronological account of the CC. Include onset, duration, frequency, and severity of symptoms. Describe the nature of symptoms, e.g., \"constantly on edge,\" \"overwhelming feelings of doom.\" Note specific episodes like panic attacks, including their duration and accompanying physical sensations like shortness of breath, increased heart rate, trembling. Detail the impact on daily life, e.g., sleep disruption – \"taking nearly an hour to fall asleep and waking frequently,\" difficulty concentrating, effects on work performance and relationships. Mention patient's attempts at coping strategies and their perceived effectiveness, e.g., \"attempting to use breathing techniques... found it challenging.\" Include direct, illustrative patient quotes if available and pertinent, e.g., \"I feel like I'm letting everyone down.\")\n",
        "            - Past Medical History (PMH): (Relevant chronic illnesses, significant past illnesses, surgeries, hospitalizations as reported by patient or mentioned in conversation).\n",
        "            - Medications: (Current medications, including name, dosage, and frequency, as reported by patient).\n",
        "            - Allergies: (Drug, food, environmental allergies and reactions, as reported by patient).\n",
        "            - Family History (FHx): (Relevant medical conditions in family members, if mentioned).\n",
        "            - Social History (SHx): (Relevant lifestyle factors, occupation, living situation, habits like smoking/alcohol, and stressors such as workplace issues, if mentioned).\n",
        "            - Review of Systems (ROS): (Briefly cover other symptoms by body system if discussed by the patient during the encounter).\n",
        "\n",
        "        **O (Objective):**\n",
        "            - General Appearance and Presentation: (e.g., \"Presented to the session on time, appropriately dressed, and well-groomed.\").\n",
        "            - Affect and Mood: (Describe observed affect, e.g., \"anxious and somewhat constricted.\" Note visible signs of emotional state like \"visible tension in her shoulders and frequent hand wringing.\").\n",
        "            - Speech: (Describe characteristics, e.g., \"rapid at times but normal in volume and content.\").\n",
        "            - Behavior and Cooperation during session: (e.g., \"maintained good eye contact throughout most of the session, though she looked down when discussing her perceived failures.\" \"receptive to feedback and actively participated.\").\n",
        "            - Cognitive Functioning: (Assess and describe, e.g., \"demonstrated intact cognitive functioning with clear and logical thought processes.\" Note orientation, memory, attention, insight).\n",
        "            - Psychomotor Activity: (e.g., normal, agitated, retarded, presence of tics or tremors).\n",
        "            - Evidence of Psychosis, Suicidal Ideation, or Homicidal Ideation: (State if assessed and the findings, e.g., \"No evidence of psychosis, suicidal ideation, or homicidal ideation was present.\").\n",
        "            - Standardized Assessment Scores: (Report any specific assessment tools used and scores, e.g., \"Completed the GAD-7 assessment with a score of 16, indicating severe anxiety symptoms.\" Note any change from previous scores if mentioned, e.g., \"increased from score of 12 at previous session.\").\n",
        "            - Vital Signs: (If measured and mentioned in the transcript, list them: e.g., BP, HR, RR, Temp, SpO2).\n",
        "            - Physical Examination Findings: (If a physical exam was performed and findings mentioned, detail them by system).\n",
        "            - Other Clinician Observations: (Any other pertinent objective observations made by the clinician during the encounter).\n",
        "\n",
        "        **A (Assessment):**\n",
        "            - Clinical Summary and Impression: (Concisely summarize the patient's current clinical status. State if they continue to meet criteria for any diagnoses, e.g., \"Alexis continues to meet criteria for Generalized Anxiety Disorder (F41.1)...\").\n",
        "            - Symptom Progression and Severity: (Note any intensification, exacerbation, stability, or improvement in symptoms since the last encounter or over the relevant period, e.g., \"symptoms that have intensified since our last session.\" \"The increase in panic attacks and sleep disturbances indicates a significant exacerbation...\").\n",
        "            - Contributing Factors and Triggers: (Identify likely triggers or factors contributing to the current presentation, e.g., \"likely triggered by increased workplace demands and her perfectionist tendencies.\").\n",
        "            - Synthesis of S & O: (Briefly explain how subjective reports and objective findings support the clinical impression. Describe any observed patterns or cycles, e.g., \"Her anxiety appears to be creating a self-reinforcing cycle, where worry about performance leads to physiological symptoms...\").\n",
        "            - Patient Strengths and Protective Factors: (Note any positive factors that can be leveraged in treatment, e.g., \"willingness to engage in therapy,\" \"her developing awareness of anxiety triggers,\" \"motivation to implement coping strategies.\").\n",
        "            - Significance of Quantitative Measures: (Reiterate any objective scores and briefly explain their clinical significance, e.g., \"Her GAD-7 score increase from 12 to 16 quantitatively confirms her subjective report of worsening symptoms.\").\n",
        "            - Areas for Therapeutic Focus: (Highlight specific challenges or areas needing therapeutic intervention, e.g., \"The client's difficulty in effectively utilizing learned coping techniques during high-stress situations suggests the need for more practice...\").\n",
        "\n",
        "        **P (Plan):**\n",
        "            (List specific, actionable items related to treatment, monitoring, and follow-up. Number each item clearly).\n",
        "            1. Therapeutic Interventions: (e.g., \"Continue weekly individual therapy sessions focusing on Cognitive Behavioral Therapy interventions for anxiety management.\").\n",
        "            2. Skill Development/Reinforcement: (e.g., \"Review and refine previously taught breathing techniques, with emphasis on practicing during periods of lower anxiety to build proficiency.\").\n",
        "            3. Introduction of New Coping Strategies: (e.g., \"Introduce progressive muscle relaxation as an additional coping strategy, with in-session demonstration and daily practice assignments.\").\n",
        "            4. Specific Therapeutic Techniques: (e.g., \"Implement thought challenging exercises targeting catastrophic thinking patterns related to work performance.\").\n",
        "            5. Patient Homework/Assignments: (e.g., \"Assign homework to create and maintain a daily anxiety journal to better identify triggers and patterns.\").\n",
        "            6. Medication Management/Consultation: (e.g., \"Discuss the potential benefits of consulting with her primary care physician regarding a medication evaluation if symptoms don't improve within 2-3 weeks.\").\n",
        "            7. Psychoeducation Topics: (e.g., \"Provide psychoeducation about the relationship between sleep hygiene and anxiety, with specific recommendations for establishing a more consistent sleep routine.\").\n",
        "            8. Follow-up Scheduling: (e.g., \"Schedule next appointment for [Date, e.g., 5/15/2025] at [Time, e.g., 10:15 am].\").\n",
        "            9. Other Referrals or Coordination of Care: (Any other planned actions, referrals, or consultations).\n",
        "\n",
        "        ---\n",
        "        Please provide the **Generated SOAP Note** strictly following this detailed structure and ensuring all relevant information from the transcript is captured in the appropriate sections:\n",
        "        \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt_content.strip()},\n",
        "        {\"role\": \"user\", \"content\": user_instructions_content.strip()}\n",
        "    ]\n",
        "\n",
        "    prompt_string = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    return prompt_string\n",
        "\n",
        "\n",
        "\n",
        "def generate_soap_note_with_llm(pipe, tokenizer_instance, prompt_text: str) -> str:\n",
        "    if not pipe or not MODEL_LOADED_SUCCESSFULLY:\n",
        "        return \"Error: LLM pipeline is not initialized or model failed to load.\"\n",
        "\n",
        "    MAX_NEW_TOKENS = 3500\n",
        "    TEMPERATURE = 0.15\n",
        "    TOP_P = 0.9\n",
        "    DO_SAMPLE = True\n",
        "\n",
        "    stop_token_ids = [\n",
        "        tokenizer_instance.eos_token_id,\n",
        "        tokenizer_instance.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "    stop_token_ids = [id for id in stop_token_ids if id is not None]\n",
        "\n",
        "\n",
        "    print(f\"Sending prompt to LLM pipeline. Max new tokens: {MAX_NEW_TOKENS}...\")\n",
        "    try:\n",
        "        outputs = pipe(\n",
        "            prompt_text,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=stop_token_ids,\n",
        "            do_sample=DO_SAMPLE,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            return_full_text=False,\n",
        "        )\n",
        "        generated_text = outputs[0]['generated_text'].strip()\n",
        "        print(\"LLM generation complete.\")\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM generation with pipeline: {e}\")\n",
        "        return f\"Error during LLM generation. Check console. Exception: {str(e)}\"\n",
        "\n",
        "\n",
        "def postprocess_soap_note(llm_output: str) -> str:\n",
        "    clean_output = llm_output.strip()\n",
        "    clean_output = clean_output.replace(\"<|eot_id|>\", \"\").replace(\"<|end_of_text|>\", \"\").strip()\n",
        "\n",
        "    if clean_output.lower().startswith(\"**generated soap note:**\"):\n",
        "        clean_output = re.split(r\"\\*\\*Generated SOAP Note:\\*\\*\", clean_output, maxsplit=1, flags=re.IGNORECASE)[-1].strip()\n",
        "    elif clean_output.lower().startswith(\"generated soap note:\"):\n",
        "        clean_output = re.split(r\"Generated SOAP Note:\", clean_output, maxsplit=1, flags=re.IGNORECASE)[-1].strip()\n",
        "\n",
        "    return clean_output\n",
        "\n",
        "\n",
        "def transcript_to_soap_pipeline(transcript_file_path: str, llm_pipe_instance, tokenizer_instance) -> str:\n",
        "    print(f\"\\n--- Starting processing for: {os.path.basename(transcript_file_path)} ---\")\n",
        "    if not MODEL_LOADED_SUCCESSFULLY or not llm_pipe_instance:\n",
        "        return \"LLM not available. Skipping.\"\n",
        "    try:\n",
        "        raw_transcript = load_transcript(transcript_file_path)\n",
        "    except Exception as e:\n",
        "        return f\"Error loading transcript: {e}\"\n",
        "\n",
        "    preprocessed_transcript = preprocess_transcript(raw_transcript)\n",
        "    prompt = create_llama3_soap_prompt(tokenizer_instance, preprocessed_transcript)\n",
        "\n",
        "    llm_generated_soap_note = generate_soap_note_with_llm(llm_pipe_instance, tokenizer_instance, prompt)\n",
        "    if \"Error during LLM generation\" in llm_generated_soap_note or llm_generated_soap_note.startswith(\"Error:\"):\n",
        "        return llm_generated_soap_note\n",
        "\n",
        "    final_soap_note = postprocess_soap_note(llm_generated_soap_note)\n",
        "    print(f\"--- Finished processing for: {os.path.basename(transcript_file_path)} ---\")\n",
        "    return final_soap_note\n",
        "\n",
        "generated_soap_notes_all = {}\n",
        "\n",
        "if not PROCEED_WITH_PROCESSING:\n",
        "    print(\"\\nSkipping SOAP note generation: Input directory issue.\")\n",
        "elif not MODEL_LOADED_SUCCESSFULLY or not LLM_PIPELINE or not TOKENIZER:\n",
        "    print(\"\\nSkipping SOAP note generation: Model or Tokenizer not loaded successfully.\")\n",
        "else:\n",
        "    print(f\"\\nStarting SOAP note generation for up to {len(TRANSCRIPT_FILENAMES)} transcripts...\")\n",
        "    print(f\"Model: {MODEL_NAME_OR_PATH} (8-bit Quantized)\")\n",
        "\n",
        "    files_processed_count = 0\n",
        "    for filename in tqdm(TRANSCRIPT_FILENAMES, desc=\"Processing Transcripts\"):\n",
        "        full_file_path = os.path.join(TRANSCRIPTS_INPUT_DIR, filename)\n",
        "        if not os.path.exists(full_file_path):\n",
        "            print(f\"\\nWarning: Transcript file {filename} not found. Skipping.\")\n",
        "            generated_soap_notes_all[filename] = f\"Error: File {filename} not found.\"\n",
        "            continue\n",
        "\n",
        "        soap_note_output = transcript_to_soap_pipeline(full_file_path, LLM_PIPELINE, TOKENIZER)\n",
        "        generated_soap_notes_all[filename] = soap_note_output\n",
        "        files_processed_count +=1\n",
        "\n",
        "        output_filepath = os.path.join(SOAP_OUTPUT_DIR, f\"soap_note_{os.path.splitext(filename)[0]}.txt\")\n",
        "        try:\n",
        "            with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "                header = (\n",
        "                    f\"SOAP Note for Transcript: {filename}\\n\"\n",
        "                    f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "                    f\"Model: {MODEL_NAME_OR_PATH} (8-bit Quantized)\\n\"\n",
        "                    f\"---\\n\\n\"\n",
        "                )\n",
        "                f.write(header)\n",
        "                f.write(soap_note_output)\n",
        "            print(f\"SOAP note for {filename} saved to: {output_filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving SOAP note for {filename}: {e}\")\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    if files_processed_count == 0:\n",
        "         print(f\"No matching transcript files processed from the list in '{TRANSCRIPTS_INPUT_DIR}'.\")\n",
        "    else:\n",
        "        print(f\"Processed {files_processed_count} transcript(s).\")\n",
        "    print(\"All specified transcripts handled.\")\n",
        "\n",
        "\n",
        "\n",
        "if PROCEED_WITH_PROCESSING and MODEL_LOADED_SUCCESSFULLY and generated_soap_notes_all:\n",
        "    first_key = next(iter(generated_soap_notes_all), None)\n",
        "    if first_key and not generated_soap_notes_all[first_key].lower().startswith(\"error:\"):\n",
        "        print(f\"\\nExample - SOAP Note for '{first_key}':\\n\")\n",
        "        print(generated_soap_notes_all[first_key])\n",
        "    elif first_key:\n",
        "        print(f\"\\nExample - Error for '{first_key}':\\n{generated_soap_notes_all[first_key]}\")\n",
        "    else:\n",
        "        print(\"\\nNo SOAP notes were generated or an error occurred that prevented display.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04ea4948e3e648729bd194aa63ebe9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31a7e41f2ca14049b6a782066372492b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39172cdda2ac44f8a78cb46cc3bd6c5e",
            "placeholder": "​",
            "style": "IPY_MODEL_7441772a308247f59b5452541ae53fc2",
            "value": " 4/4 [01:22&lt;00:00, 17.62s/it]"
          }
        },
        "39172cdda2ac44f8a78cb46cc3bd6c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef4f700a16448aea0bfd779070272c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60526cb2ce874e18b0317cfef69a2a35",
              "IPY_MODEL_f417fa49b33544b8a3e1c85fca6f00fe",
              "IPY_MODEL_31a7e41f2ca14049b6a782066372492b"
            ],
            "layout": "IPY_MODEL_705544c2a786439b84b3ca01bb342105"
          }
        },
        "60526cb2ce874e18b0317cfef69a2a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be51213081f42fa9c2539d772eeb40f",
            "placeholder": "​",
            "style": "IPY_MODEL_04ea4948e3e648729bd194aa63ebe9e1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "705544c2a786439b84b3ca01bb342105": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7441772a308247f59b5452541ae53fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9be51213081f42fa9c2539d772eeb40f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae77e20ea7414f928d49b3484fb696f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf0d01ec4f2426fa7e0eb9f07bd3492": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f417fa49b33544b8a3e1c85fca6f00fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae77e20ea7414f928d49b3484fb696f5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eaf0d01ec4f2426fa7e0eb9f07bd3492",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
